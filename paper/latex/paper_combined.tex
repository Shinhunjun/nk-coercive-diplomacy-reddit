% Combined LaTeX file - ICWSM 2025 Paper
% All sections merged into single file for reference
% Based on official AAAI/ICWSM template
\documentclass[letterpaper]{article}
\usepackage[submission]{aaai25}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage[hyphens]{url}
\usepackage{graphicx}
\urlstyle{rm}
\def\UrlFont{\rm}
\usepackage{natbib}
\usepackage{caption}
\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}

% Additional allowed packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{stfloats}

% Checklist macros
\newcommand{\answerYes}[1]{\textcolor{blue}{#1}} 
\newcommand{\answerNo}[1]{\textcolor{teal}{#1}} 
\newcommand{\answerNA}[1]{\textcolor{gray}{#1}} 
\newcommand{\answerTODO}[1]{\textcolor{red}{#1}} 

% PDF Info
\pdfinfo{
/Title (Summit Diplomacy and Social Media Framing)
/Author (Anonymous Submission)
/TemplateVersion (2025.1)
}

\setcounter{secnumdepth}{0}

% Title
\title{Summit Diplomacy and Social Media Framing: \\
A Causal Analysis of U.S.-North Korea Summits on Reddit Public Opinion}

% For anonymous submission
\author{
    Anonymous Submission
}
\affiliations{
    
}

\begin{document}

\maketitle

%==============================================================================
% ABSTRACT
%==============================================================================
\begin{abstract}
High-stakes diplomatic summits can dramatically shift public perception of foreign adversaries, yet the causal mechanisms underlying these shifts remain poorly understood in online discourse. This study examines how the 2018 Singapore Summit and the 2019 Hanoi Summit failure affected public opinion framing of North Korea on Reddit. Using a novel combination of LLM-based framing classification validated by domain experts (military officers) and a Difference-in-Differences design with multiple control countries (China, Iran, Russia), we analyze over 35,000 Reddit posts from 2017-2019. 

Our findings reveal that the Singapore Summit produced a significant positive shift in both sentiment (+0.10 to +0.21) and framing (+0.85 to +1.28) toward diplomatic frames, while the failed Hanoi Summit triggered a partial reversal (-0.06 to -0.12 in sentiment, -0.30 to -0.88 in framing). Notably, the positive gains from successful diplomacy were not fully erased by the diplomatic failure, suggesting a ``ratchet effect'' in public opinion formation.

This work contributes to understanding the ``rally-round-the-flag'' effect in digital spaces and demonstrates a rigorous methodology for causal inference in computational social science.
\end{abstract}

%==============================================================================
% INTRODUCTION
%==============================================================================
\section{Introduction}

The historic 2018 Singapore Summit between U.S. President Donald Trump and North Korean leader Kim Jong Un marked the first-ever meeting between sitting leaders of the two nations. This diplomatic breakthrough, followed by the collapsed Hanoi Summit in February 2019, provides a unique natural experiment to study how high-stakes international events shape online public opinion.

\subsection{Research Questions}

We investigate the causal impact of diplomatic summits on public opinion through two complementary dimensions:

\begin{itemize}
    \item \textbf{RQ1 (Content Change)}: How do high-stakes diplomatic summits affect the \textit{content} of public opinion (framing and sentiment) toward adversary nations?
    \item \textbf{RQ2 (Structural Change)}: How does the underlying \textit{network structure} of discourse change across different phases of diplomatic engagement?
\end{itemize}

\subsection{Contributions}

\begin{enumerate}
    \item \textbf{Methodological}: A validated pipeline combining LLM-based framing analysis with human expert annotation (military officers).
    \item \textbf{Empirical}: Causal evidence of summit effects on adversary perception using Difference-in-Differences with multiple controls.
    \item \textbf{Theoretical}: Evidence of asymmetric effects---diplomatic successes produce larger gains than failures produce reversals.
\end{enumerate}

%==============================================================================
% RELATED WORK
%==============================================================================
\section{Related Work}

\subsection{Rally-Round-the-Flag Effect}

The rally-round-the-flag effect describes the phenomenon where public approval of national leaders increases during international crises or major foreign policy events \cite{mueller1970}. While extensively studied in traditional polling contexts, its manifestation in social media discourse remains underexplored.

\subsection{Media Framing}

Framing theory posits that how information is presented significantly influences audience interpretation \cite{entman1993}. In international relations coverage, common frames include threat, diplomacy, economic, and humanitarian perspectives \cite{iyengar1991}.

\subsection{Social Media and Public Opinion}

Reddit, as a major discussion platform, has been increasingly used to study public opinion dynamics \cite{baumgartner2020}. Unlike Twitter, Reddit's structure enables longer-form discussion and topical organization through subreddits.

\subsection{LLMs for Content Analysis}

Recent work has demonstrated the effectiveness of large language models for text classification tasks previously requiring human annotation \cite{gilardi2023}. However, validation against domain experts remains critical for specialized content.

%==============================================================================
% DATA
%==============================================================================
\section{Data}

\subsection{Data Collection}

We collected Reddit posts from January 2017 to December 2019 using the Arctic Shift API, targeting major subreddits: r/worldnews, r/politics, r/news, r/geopolitics, and r/NeutralPolitics.

\subsection{Treatment and Control Groups}

\begin{table}[t]
\centering
\setlength{\tabcolsep}{3pt}
\caption{Dataset Overview}
\begin{tabular}{llrl}
\hline
\textbf{Group} & \textbf{Topic} & \textbf{N} & \textbf{Role} \\
\hline
Treatment & N. Korea & 10,659 & Primary \\
Control 1 & China & 8,024 & Trade confounder \\
Control 2 & Iran & 5,983 & Nuclear comparison \\
Control 3 & Russia & 10,261 & Investigation \\
\hline
\end{tabular}
\label{tab:dataset}
\end{table}

\subsection{Key Events Timeline}

\begin{table}[t]
\centering
\setlength{\tabcolsep}{3pt}
\caption{Key Events}
\begin{tabular}{lll}
\hline
\textbf{Date} & \textbf{Event} & \textbf{Effect} \\
\hline
2018-03-08 & Summit Announced & Positive shift \\
2018-06-12 & Singapore Summit & Peak diplomacy \\
2019-02-27 & Hanoi (Failed) & Reversal \\
\hline
\end{tabular}
\label{tab:events}
\end{table}

\subsection{Analysis Periods}

To avoid anticipation effects, we exclude transition months:

\begin{itemize}
    \item \textbf{P1} (2017.01--2018.02): Pre-Announcement
    \item \textbf{P2} (2018.06--2019.01): Singapore-Hanoi
    \item \textbf{P3} (2019.03--2019.12): Post-Hanoi
    \item \textit{Excluded}: 2018.03--05, 2019.02 (Transition)
\end{itemize}

%==============================================================================
% METHOD
%==============================================================================
\section{Method}

\subsection{Human Annotation Benchmark}

To validate our LLM-based framing classification, we developed a rigorous human annotation benchmark following established protocols in computational social science.

\subsubsection{Annotators}
Two military officers with expertise in North Korean affairs and international security independently labeled posts. Both annotators have domain knowledge essential for accurately interpreting geopolitical discourse.

\subsubsection{Sampling Strategy}
We employed stratified proportional sampling to create a representative validation dataset of 1,330 Reddit posts:

\begin{itemize}
    \item \textbf{Countries}: North Korea (n=335), China (n=337), Iran (n=337), Russia (n=321)
    \item \textbf{Time Periods}: P1 Pre-Singapore (n=512), P2 Singapore-Hanoi (n=256), P3 Post-Hanoi (n=562)
    \item \textbf{Total Strata}: 60 (4 countries $\times$ 3 periods $\times$ 5 frames)
    \item \textbf{Minimum per stratum}: 10 posts to ensure rare category representation
\end{itemize}

\subsubsection{Frame Categories}
Following the same definitions used for LLM classification:
\begin{itemize}
    \item \textbf{THREAT} (-2): Military threat, nuclear weapons, missiles, war risk
    \item \textbf{ECONOMIC} (-1): Economic sanctions, trade aspects
    \item \textbf{NEUTRAL} (0): Neutral information delivery
    \item \textbf{HUMANITARIAN} (+1): Human rights, refugees, civilian issues
    \item \textbf{DIPLOMACY} (+2): Negotiation, dialogue, peace, cooperation
\end{itemize}

\subsubsection{Annotation Process}
We followed an iterative codebook refinement approach:
\begin{enumerate}
    \item \textbf{Pilot Round}: Both annotators independently labeled 100 posts, calculated initial inter-rater reliability, discussed disagreements, and refined the codebook with edge case decision rules.
    \item \textbf{Main Annotation}: Remaining 1,230 posts annotated in batches with ongoing IRR monitoring to ensure consistent quality.
    \item \textbf{Disagreement Resolution}: All disagreements resolved through discussion to produce final consensus labels (not discarding disagreements).
\end{enumerate}

\subsubsection{Inter-Rater Reliability}
We report Cohen's $\kappa$ as our primary reliability measure:
\begin{itemize}
    \item \textbf{Initial Agreement}: $\kappa$ = [TBD] (before resolution)
    \item \textbf{Interpretation}: [substantial/almost perfect] agreement per Landis \& Koch (1977)
\end{itemize}

\subsection{LLM-Based Framing Classification}

We use GPT-4o-mini for frame classification with zero-shot prompting. The model classifies each post into one of five categories with a confidence score.

\subsection{Sentiment Analysis}

We employ RoBERTa-based sentiment model\footnote{cardiffnlp/twitter-roberta-base-sentiment-latest} for sentiment analysis:
\begin{itemize}
    \item \textbf{Scale}: Continuous score from -1 (Negative) to +1 (Positive)
    \item \textbf{Rationale}: While framing captures \textit{how} topics are discussed, sentiment captures the \textit{emotional valence}
\end{itemize}

\subsection{Difference-in-Differences Design}

Our DID specification:

\begin{equation}
Y_{it} = \alpha + \beta_1 \text{Post}_t + \beta_2 \text{Treatment}_i + \beta_3 (\text{Post}_t \times \text{Treatment}_i) + \epsilon_{it}
\end{equation}

where $\beta_3$ represents the causal effect of the summit on framing/sentiment.

\subsubsection{Parallel Trends Validation}

Before DID estimation, we verify the parallel trends assumption using monthly aggregated data:

\begin{equation}
Y_{it} = \alpha + \beta_1 \text{Time}_t + \beta_2 \text{Treatment}_i + \beta_3 (\text{Time}_t \times \text{Treatment}_i) + \epsilon_{it}
\end{equation}

A significant $\beta_3$ (p < 0.05) indicates violation of the parallel trends assumption.

%==============================================================================
% RESULTS
%==============================================================================
\section{Results}

% Timeline figure at the beginning
\begin{figure*}[t]
\centering
\includegraphics[width=0.9\textwidth]{figures/fig1_timeline.png}
\caption{Research Timeline and Key Events. Shaded regions indicate analysis periods: P1 (gray), Transition (yellow), P2 (green), P3 (red).}
\label{fig:timeline}
\end{figure*}

\subsection{RQ1: Content Changes (Sentiment \& Framing)}

\subsubsection{Impact on Sentiment}

\begin{table}[t]
\centering
\caption{Parallel Trends Test Results (Sentiment)}
\begin{tabular}{llcc}
\hline
\textbf{Comparison} & \textbf{Control} & \textbf{P-value} & \textbf{OK} \\
\hline
P1$\rightarrow$P2 & China & 0.99 & \checkmark \\
P1$\rightarrow$P2 & Iran & 0.83 & \checkmark \\
P2$\rightarrow$P3 & China & 0.69 & \checkmark \\
P2$\rightarrow$P3 & Iran & 0.75 & \checkmark \\
\hline
\end{tabular}
\label{tab:pt_sentiment}
\end{table}

The Singapore Summit (P1$\rightarrow$P2) produced a significant positive shift in sentiment across all control groups (+0.10 to +0.21, p < 0.001). Conversely, the Hanoi Summit failure (P2$\rightarrow$P3) resulted in a negative shift (-0.06 to -0.12, p < 0.001).

% Sentiment trends figure near sentiment results
\begin{figure}[t]
\centering
\includegraphics[width=0.95\columnwidth]{figures/fig6_sentiment_trends.png}
\caption{Monthly Sentiment Score Trends}
\label{fig:sentiment_trends}
\end{figure}

\subsubsection{Impact on Framing}

We observed similar patterns in framing, where positive values indicate diplomatic framing and negative values indicate threat framing.

% Figure 4 moved up here to ensure it appears on the correct page
\begin{figure*}[t]
\centering
\includegraphics[width=0.9\textwidth]{figures/fig4_did_visualization.png}
\caption{Difference-in-Differences Visualization. (A) Singapore Summit Effect, (B) Hanoi Summit Effect.}
\label{fig:did}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[width=0.9\textwidth]{figures/fig2_framing_trends.png}
\caption{Monthly Framing Score Trends: NK vs Control Groups. Vertical lines mark key events.}
\label{fig:framing_trends}
\end{figure*}

\begin{table}[t]
\centering
\caption{Framing DID Results Summary}
\begin{tabular}{llcc}
\hline
\textbf{Event} & \textbf{Control} & \textbf{DID Est.} & \textbf{P-val} \\
\hline
Singapore & China & \textbf{+1.28} & $<$0.001 \\
(P1$\rightarrow$P2) & Iran & \textbf{+0.85} & $<$0.001 \\
\hline
Hanoi & China & \textbf{-0.88} & $<$0.001 \\
(P2$\rightarrow$P3) & Iran & \textbf{-0.30} & 0.003 \\
\hline
\end{tabular}
\label{tab:did_framing_summary}
\end{table}

The Singapore Summit led to a strong shift toward diplomatic frames (+0.85 to +1.28), while the Hanoi Summit caused a partial reversion toward threat frames (-0.30 to -0.88).

\subsection{RQ2: Structural Changes (GraphRAG)}

[TO BE COMPLETED: GraphRAG Analysis Results]

%==============================================================================
% DISCUSSION
%==============================================================================
\section{Discussion}

\subsection{Asymmetry of Diplomatic Effects}

Our findings reveal an asymmetric pattern in how diplomatic events affect public opinion. The Singapore Summit produced positive shifts in both sentiment (+0.10 to +0.21) and framing (+0.85 to +1.28), while the Hanoi failure caused smaller reversals in sentiment (-0.06 to -0.12) and framing (-0.30 to -0.88).

This asymmetry suggests that successful diplomatic engagement creates durable changes in public discourse that are not fully reversed by subsequent failures---a phenomenon we term the ``ratchet effect'' in public opinion.

\subsection{Framing vs Sentiment}

An important finding is the differential magnitude of effects across our two measures:

\begin{itemize}
    \item \textbf{Framing}: Captures approximately 25-32\% of scale change (DID $\approx$ 1.0 on a -2 to +2 scale)
    \item \textbf{Sentiment}: Captures approximately 5-10\% of scale change (DID $\approx$ 0.1 on a -1 to +1 scale)
\end{itemize}

This suggests that framing---how topics are discussed---is more responsive to diplomatic events than emotional valence.

\subsection{Control Group Selection}

Our multi-control design reveals the importance of control group selection:

\begin{itemize}
    \item \textbf{Iran}: Satisfies parallel trends for both sentiment and framing across all comparisons
    \item \textbf{China}: Satisfies parallel trends with clean period definitions
    \item \textbf{Russia}: Shows some violations, particularly for framing P1$\rightarrow$P2
\end{itemize}

The consistency of results across controls that satisfy parallel trends strengthens causal interpretation.

\subsection{Implications for Policy Communication}

Our findings have implications for understanding how diplomatic initiatives are received by the public:

\begin{enumerate}
    \item Summit meetings can rapidly shift public discourse framing
    \item Failed negotiations do not fully undo positive gains
    \item Social media sentiment may lag behind framing changes
\end{enumerate}

\subsection{Limitations}

\begin{itemize}
    \item \textbf{Platform specificity}: Reddit may not represent broader public opinion
    \item \textbf{LLM classification}: While validated, potential for systematic biases
    \item \textbf{Confounder control}: Other events during study period may affect results
    \item \textbf{External validity}: Findings may be specific to U.S.-NK context
\end{itemize}

%==============================================================================
% CONCLUSION
%==============================================================================
\section{Conclusion}

This study provides causal evidence of how high-stakes diplomatic summits affect public opinion framing on social media. Using a Difference-in-Differences design with validated LLM-based classification, we find that:

\begin{enumerate}
    \item The 2018 Singapore Summit produced significant positive shifts in both sentiment and framing toward North Korea
    \item The 2019 Hanoi Summit failure triggered partial reversals, but did not fully undo the positive gains
    \item This ``ratchet effect'' suggests that successful diplomacy can create durable changes in public discourse
\end{enumerate}

Our methodology---combining LLM classification with expert validation and rigorous causal inference---provides a template for studying public opinion dynamics around international events. Future work should extend this analysis to other diplomatic contexts and examine the mechanisms underlying opinion persistence.

%==============================================================================
% REFERENCES
%==============================================================================
\bibliography{references}

%==============================================================================
% PAPER CHECKLIST (Required for ICWSM submission)
%==============================================================================
\newpage
\section*{Paper Checklist}

\begin{enumerate}

\item For most authors...
\begin{enumerate}
    \item Would answering this research question advance science without violating social contracts?
    \answerYes{Yes, this study examines public discourse and does not involve private data.}
    
    \item Do your main claims accurately reflect the paper's contributions?
    \answerYes{Yes, see Abstract and Section 1.}
    
    \item Do you clarify how the proposed methodological approach is appropriate?
    \answerYes{Yes, see Section 4 (Method) for DID justification.}
    
    \item Do you clarify what are possible artifacts in the data?
    \answerYes{Yes, we discuss platform-specific limitations in Section 6.}
    
    \item Did you describe the limitations of your work?
    \answerYes{Yes, see Section 6 (Discussion - Limitations).}
    
    \item Did you discuss potential negative societal impacts?
    \answerNA{NA, this is observational research on public discourse.}
    
    \item Did you discuss potential misuse of your work?
    \answerNA{NA, findings describe public opinion patterns.}
    
    \item Did you describe steps to prevent negative outcomes?
    \answerYes{Yes, we use aggregated data and do not identify individuals.}
    
    \item Have you ensured your paper conforms to ethics guidelines?
    \answerYes{Yes.}
\end{enumerate}

\item Additionally, if your study involves hypothesis testing...
\begin{enumerate}
    \item Did you clearly state assumptions?
    \answerYes{Yes, parallel trends assumption in Section 4 and 5.}
    
    \item Have you provided justifications for theoretical results?
    \answerYes{Yes, we validate assumptions empirically.}
    
    \item Did you discuss competing hypotheses?
    \answerYes{Yes, we use multiple control groups for robustness.}
    
    \item Have you considered alternative explanations?
    \answerYes{Yes, discussed in Section 6.}
    
    \item Did you address potential biases?
    \answerYes{Yes, see Limitations in Section 6.}
    
    \item Have you related results to existing literature?
    \answerYes{Yes, see Section 2 (Related Work).}
    
    \item Did you discuss implications?
    \answerYes{Yes, see Section 6 (Discussion).}
\end{enumerate}

\item Additionally, if you ran machine learning experiments...
\begin{enumerate}
    \item Did you include code and data?
    \answerTODO{Will be provided upon acceptance.}
    
    \item Did you specify training details?
    \answerYes{Yes, we specify LLM model and prompts in Section 4.}
    
    \item Did you report error bars?
    \answerYes{Yes, standard errors provided in DID tables.}
    
    \item Did you include compute resources?
    \answerYes{GPT-4o-mini API, RoBERTa on local GPU.}
    
    \item Do you justify evaluation appropriateness?
    \answerYes{Yes, we validate LLM against human experts.}
    
    \item Did you discuss misclassification costs?
    \answerYes{Yes, see validation agreement metrics.}
\end{enumerate}

\item Additionally, if using existing assets...
\begin{enumerate}
    \item Did you cite the creators?
    \answerYes{Yes, Arctic Shift API and model citations included.}
    
    \item Did you mention licenses?
    \answerYes{Reddit data via public API; models via standard licenses.}
    
    \item Did you include new assets?
    \answerTODO{Will provide framing-annotated dataset.}
    
    \item Did you discuss consent?
    \answerYes{Reddit posts are public; no identifiable information used.}
    
    \item Does data contain PII?
    \answerNo{No, we use only public posts with anonymized usernames.}
\end{enumerate}

\item Additionally, if you used crowdsourcing or human subjects...
\begin{enumerate}
    \item Did you include instructions?
    \answerYes{Yes, annotation guidelines described in Section 4.}
    
    \item Did you describe participant risks?
    \answerNA{NA, annotators are expert collaborators, not research subjects.}
    
    \item Did you include hourly wage?
    \answerNA{NA, annotators are collaborating researchers.}
    
    \item Did you discuss data storage?
    \answerYes{Annotations stored locally.}
\end{enumerate}

\end{enumerate}

\end{document}
