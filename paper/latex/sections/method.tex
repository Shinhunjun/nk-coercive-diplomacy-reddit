% Method
\section{Method}

\subsection{Human Annotation Benchmark}

To validate our LLM-based framing classification, we developed a rigorous human annotation benchmark following established protocols in computational social science.

\subsubsection{Annotators}
Two military officers with expertise in North Korean affairs and international security independently labeled posts. Both annotators have domain knowledge essential for accurately interpreting geopolitical discourse.

\subsubsection{Sampling Strategy}
We employed stratified proportional sampling to create a representative validation dataset of 1,330 Reddit posts:

\begin{itemize}
    \item \textbf{Countries}: North Korea (n=335), China (n=337), Iran (n=337), Russia (n=321)
    \item \textbf{Time Periods}: P1 Pre-Singapore (n=512), P2 Singapore-Hanoi (n=256), P3 Post-Hanoi (n=562)
    \item \textbf{Total Strata}: 60 (4 countries $\times$ 3 periods $\times$ 5 frames)
    \item \textbf{Minimum per stratum}: 10 posts to ensure rare category representation
\end{itemize}

\subsubsection{Frame Categories}
Following the same definitions used for LLM classification:
\begin{itemize}
    \item \textbf{THREAT} (-2): Military threat, nuclear weapons, missiles, war risk
    \item \textbf{ECONOMIC} (-1): Economic sanctions, trade aspects
    \item \textbf{NEUTRAL} (0): Neutral information delivery
    \item \textbf{HUMANITARIAN} (+1): Human rights, refugees, civilian issues
    \item \textbf{DIPLOMACY} (+2): Negotiation, dialogue, peace, cooperation
\end{itemize}

\subsubsection{Annotation Process}
We followed an iterative codebook refinement approach:
\begin{enumerate}
    \item \textbf{Pilot Round}: Both annotators independently labeled 100 posts, calculated initial inter-rater reliability, discussed disagreements, and refined the codebook with edge case decision rules.
    \item \textbf{Main Annotation}: Remaining 1,230 posts annotated in batches with ongoing IRR monitoring to ensure consistent quality.
    \item \textbf{Disagreement Resolution}: All disagreements resolved through discussion to produce final consensus labels (not discarding disagreements).
\end{enumerate}

\subsubsection{Inter-Rater Reliability}
We report Cohen's $\kappa$ as our primary reliability measure:
\begin{itemize}
    \item \textbf{Initial Agreement}: $\kappa$ = [TBD] (before resolution)
    \item \textbf{Interpretation}: [substantial/almost perfect] agreement per Landis \& Koch (1977)
\end{itemize}

\subsection{LLM-Based Framing Classification}

We use GPT-4o-mini for frame classification with zero-shot prompting. The model classifies each post into one of five categories with a confidence score.

\subsection{Sentiment Analysis}

We employ RoBERTa-based sentiment model\footnote{cardiffnlp/twitter-roberta-base-sentiment-latest} for sentiment analysis:
\begin{itemize}
    \item \textbf{Scale}: Continuous score from -1 (Negative) to +1 (Positive)
    \item \textbf{Rationale}: While framing captures \textit{how} topics are discussed, sentiment captures the \textit{emotional valence}
\end{itemize}

\subsection{Difference-in-Differences Design}

Our DID specification:

\begin{equation}
Y_{it} = \alpha + \beta_1 \text{Post}_t + \beta_2 \text{Treatment}_i + \beta_3 (\text{Post}_t \times \text{Treatment}_i) + \epsilon_{it}
\end{equation}

where $\beta_3$ represents the causal effect of the summit on framing/sentiment.

\subsubsection{Parallel Trends Validation}

Before DID estimation, we verify the parallel trends assumption using monthly aggregated data:

\begin{equation}
Y_{it} = \alpha + \beta_1 \text{Time}_t + \beta_2 \text{Treatment}_i + \beta_3 (\text{Time}_t \times \text{Treatment}_i) + \epsilon_{it}
\end{equation}

A significant $\beta_3$ (p < 0.05) indicates violation of the parallel trends assumption.
